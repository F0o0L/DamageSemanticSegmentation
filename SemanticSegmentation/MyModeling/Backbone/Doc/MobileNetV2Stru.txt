/home/fool/develop/conda/bin/python /home/fool/develop/Workplace/MyModel/MyModeling/Backbone/MyMobileNetV2.py
torch.Size([1, 1280, 6, 8])
MobileNetV2(
  (features): Sequential(
    (0): ConvBNActivation(
      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
    )
    (1): InvertedResidual(
      (conv): Sequential(
        (0): ConvBNActivation(
          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): InvertedResidual(
      (conv): Sequential(
        (0): ConvBNActivation(
          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): ConvBNActivation(
          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): InvertedResidual(
      (conv): Sequential(
        (0): ConvBNActivation(
          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): ConvBNActivation(
          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)
          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): InvertedResidual(
      (conv): Sequential(
        (0): ConvBNActivation(
          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): ConvBNActivation(
          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): InvertedResidual(
      (conv): Sequential(
        (0): ConvBNActivation(
          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): ConvBNActivation(
          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): InvertedResidual(
      (conv): Sequential(
        (0): ConvBNActivation(
          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): ConvBNActivation(
          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (7): InvertedResidual(
      (conv): Sequential(
        (0): ConvBNActivation(
          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): ConvBNActivation(
          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (8): InvertedResidual(
      (conv): Sequential(
        (0): ConvBNActivation(
          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): ConvBNActivation(
          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (9): InvertedResidual(
      (conv): Sequential(
        (0): ConvBNActivation(
          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): ConvBNActivation(
          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (10): InvertedResidual(
      (conv): Sequential(
        (0): ConvBNActivation(
          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): ConvBNActivation(
          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (11): InvertedResidual(
      (conv): Sequential(
        (0): ConvBNActivation(
          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): ConvBNActivation(
          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (12): InvertedResidual(
      (conv): Sequential(
        (0): ConvBNActivation(
          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): ConvBNActivation(
          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (13): InvertedResidual(
      (conv): Sequential(
        (0): ConvBNActivation(
          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): ConvBNActivation(
          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (14): InvertedResidual(
      (conv): Sequential(
        (0): ConvBNActivation(
          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): ConvBNActivation(
          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (15): InvertedResidual(
      (conv): Sequential(
        (0): ConvBNActivation(
          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): ConvBNActivation(
          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=960, bias=False)
          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (16): InvertedResidual(
      (conv): Sequential(
        (0): ConvBNActivation(
          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): ConvBNActivation(
          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=960, bias=False)
          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (17): InvertedResidual(
      (conv): Sequential(
        (0): ConvBNActivation(
          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): ConvBNActivation(
          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=960, bias=False)
          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (18): ConvBNActivation(
      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), dilation=(2, 2), bias=False)
      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
    )
  )
)
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 32, 256, 256]             864
       BatchNorm2d-2         [-1, 32, 256, 256]              64
             ReLU6-3         [-1, 32, 256, 256]               0
            Conv2d-4         [-1, 32, 256, 256]             288
       BatchNorm2d-5         [-1, 32, 256, 256]              64
             ReLU6-6         [-1, 32, 256, 256]               0
            Conv2d-7         [-1, 16, 256, 256]             512
       BatchNorm2d-8         [-1, 16, 256, 256]              32
  InvertedResidual-9         [-1, 16, 256, 256]               0
           Conv2d-10         [-1, 96, 256, 256]           1,536
      BatchNorm2d-11         [-1, 96, 256, 256]             192
            ReLU6-12         [-1, 96, 256, 256]               0
           Conv2d-13         [-1, 96, 128, 128]             864
      BatchNorm2d-14         [-1, 96, 128, 128]             192
            ReLU6-15         [-1, 96, 128, 128]               0
           Conv2d-16         [-1, 24, 128, 128]           2,304
      BatchNorm2d-17         [-1, 24, 128, 128]              48
 InvertedResidual-18         [-1, 24, 128, 128]               0
           Conv2d-19        [-1, 144, 128, 128]           3,456
      BatchNorm2d-20        [-1, 144, 128, 128]             288
            ReLU6-21        [-1, 144, 128, 128]               0
           Conv2d-22        [-1, 144, 128, 128]           1,296
      BatchNorm2d-23        [-1, 144, 128, 128]             288
            ReLU6-24        [-1, 144, 128, 128]               0
           Conv2d-25         [-1, 24, 128, 128]           3,456
      BatchNorm2d-26         [-1, 24, 128, 128]              48
 InvertedResidual-27         [-1, 24, 128, 128]               0
           Conv2d-28        [-1, 144, 128, 128]           3,456
      BatchNorm2d-29        [-1, 144, 128, 128]             288
            ReLU6-30        [-1, 144, 128, 128]               0
           Conv2d-31          [-1, 144, 64, 64]           1,296
      BatchNorm2d-32          [-1, 144, 64, 64]             288
            ReLU6-33          [-1, 144, 64, 64]               0
           Conv2d-34           [-1, 32, 64, 64]           4,608
      BatchNorm2d-35           [-1, 32, 64, 64]              64
 InvertedResidual-36           [-1, 32, 64, 64]               0
           Conv2d-37          [-1, 192, 64, 64]           6,144
      BatchNorm2d-38          [-1, 192, 64, 64]             384
            ReLU6-39          [-1, 192, 64, 64]               0
           Conv2d-40          [-1, 192, 64, 64]           1,728
      BatchNorm2d-41          [-1, 192, 64, 64]             384
            ReLU6-42          [-1, 192, 64, 64]               0
           Conv2d-43           [-1, 32, 64, 64]           6,144
      BatchNorm2d-44           [-1, 32, 64, 64]              64
 InvertedResidual-45           [-1, 32, 64, 64]               0
           Conv2d-46          [-1, 192, 64, 64]           6,144
      BatchNorm2d-47          [-1, 192, 64, 64]             384
            ReLU6-48          [-1, 192, 64, 64]               0
           Conv2d-49          [-1, 192, 64, 64]           1,728
      BatchNorm2d-50          [-1, 192, 64, 64]             384
            ReLU6-51          [-1, 192, 64, 64]               0
           Conv2d-52           [-1, 32, 64, 64]           6,144
      BatchNorm2d-53           [-1, 32, 64, 64]              64
 InvertedResidual-54           [-1, 32, 64, 64]               0
           Conv2d-55          [-1, 192, 64, 64]           6,144
      BatchNorm2d-56          [-1, 192, 64, 64]             384
            ReLU6-57          [-1, 192, 64, 64]               0
           Conv2d-58          [-1, 192, 32, 32]           1,728
      BatchNorm2d-59          [-1, 192, 32, 32]             384
            ReLU6-60          [-1, 192, 32, 32]               0
           Conv2d-61           [-1, 64, 32, 32]          12,288
      BatchNorm2d-62           [-1, 64, 32, 32]             128
 InvertedResidual-63           [-1, 64, 32, 32]               0
           Conv2d-64          [-1, 384, 32, 32]          24,576
      BatchNorm2d-65          [-1, 384, 32, 32]             768
            ReLU6-66          [-1, 384, 32, 32]               0
           Conv2d-67          [-1, 384, 32, 32]           3,456
      BatchNorm2d-68          [-1, 384, 32, 32]             768
            ReLU6-69          [-1, 384, 32, 32]               0
           Conv2d-70           [-1, 64, 32, 32]          24,576
      BatchNorm2d-71           [-1, 64, 32, 32]             128
 InvertedResidual-72           [-1, 64, 32, 32]               0
           Conv2d-73          [-1, 384, 32, 32]          24,576
      BatchNorm2d-74          [-1, 384, 32, 32]             768
            ReLU6-75          [-1, 384, 32, 32]               0
           Conv2d-76          [-1, 384, 32, 32]           3,456
      BatchNorm2d-77          [-1, 384, 32, 32]             768
            ReLU6-78          [-1, 384, 32, 32]               0
           Conv2d-79           [-1, 64, 32, 32]          24,576
      BatchNorm2d-80           [-1, 64, 32, 32]             128
 InvertedResidual-81           [-1, 64, 32, 32]               0
           Conv2d-82          [-1, 384, 32, 32]          24,576
      BatchNorm2d-83          [-1, 384, 32, 32]             768
            ReLU6-84          [-1, 384, 32, 32]               0
           Conv2d-85          [-1, 384, 32, 32]           3,456
      BatchNorm2d-86          [-1, 384, 32, 32]             768
            ReLU6-87          [-1, 384, 32, 32]               0
           Conv2d-88           [-1, 64, 32, 32]          24,576
      BatchNorm2d-89           [-1, 64, 32, 32]             128
 InvertedResidual-90           [-1, 64, 32, 32]               0
           Conv2d-91          [-1, 384, 32, 32]          24,576
      BatchNorm2d-92          [-1, 384, 32, 32]             768
            ReLU6-93          [-1, 384, 32, 32]               0
           Conv2d-94          [-1, 384, 32, 32]           3,456
      BatchNorm2d-95          [-1, 384, 32, 32]             768
            ReLU6-96          [-1, 384, 32, 32]               0
           Conv2d-97           [-1, 96, 32, 32]          36,864
      BatchNorm2d-98           [-1, 96, 32, 32]             192
 InvertedResidual-99           [-1, 96, 32, 32]               0
          Conv2d-100          [-1, 576, 32, 32]          55,296
     BatchNorm2d-101          [-1, 576, 32, 32]           1,152
           ReLU6-102          [-1, 576, 32, 32]               0
          Conv2d-103          [-1, 576, 32, 32]           5,184
     BatchNorm2d-104          [-1, 576, 32, 32]           1,152
           ReLU6-105          [-1, 576, 32, 32]               0
          Conv2d-106           [-1, 96, 32, 32]          55,296
     BatchNorm2d-107           [-1, 96, 32, 32]             192
InvertedResidual-108           [-1, 96, 32, 32]               0
          Conv2d-109          [-1, 576, 32, 32]          55,296
     BatchNorm2d-110          [-1, 576, 32, 32]           1,152
           ReLU6-111          [-1, 576, 32, 32]               0
          Conv2d-112          [-1, 576, 32, 32]           5,184
     BatchNorm2d-113          [-1, 576, 32, 32]           1,152
           ReLU6-114          [-1, 576, 32, 32]               0
          Conv2d-115           [-1, 96, 32, 32]          55,296
     BatchNorm2d-116           [-1, 96, 32, 32]             192
InvertedResidual-117           [-1, 96, 32, 32]               0
          Conv2d-118          [-1, 576, 32, 32]          55,296
     BatchNorm2d-119          [-1, 576, 32, 32]           1,152
           ReLU6-120          [-1, 576, 32, 32]               0
          Conv2d-121          [-1, 576, 16, 16]           5,184
     BatchNorm2d-122          [-1, 576, 16, 16]           1,152
           ReLU6-123          [-1, 576, 16, 16]               0
          Conv2d-124          [-1, 160, 16, 16]          92,160
     BatchNorm2d-125          [-1, 160, 16, 16]             320
InvertedResidual-126          [-1, 160, 16, 16]               0
          Conv2d-127          [-1, 960, 16, 16]         153,600
     BatchNorm2d-128          [-1, 960, 16, 16]           1,920
           ReLU6-129          [-1, 960, 16, 16]               0
          Conv2d-130          [-1, 960, 16, 16]           8,640
     BatchNorm2d-131          [-1, 960, 16, 16]           1,920
           ReLU6-132          [-1, 960, 16, 16]               0
          Conv2d-133          [-1, 160, 16, 16]         153,600
     BatchNorm2d-134          [-1, 160, 16, 16]             320
InvertedResidual-135          [-1, 160, 16, 16]               0
          Conv2d-136          [-1, 960, 16, 16]         153,600
     BatchNorm2d-137          [-1, 960, 16, 16]           1,920
           ReLU6-138          [-1, 960, 16, 16]               0
          Conv2d-139          [-1, 960, 16, 16]           8,640
     BatchNorm2d-140          [-1, 960, 16, 16]           1,920
           ReLU6-141          [-1, 960, 16, 16]               0
          Conv2d-142          [-1, 160, 16, 16]         153,600
     BatchNorm2d-143          [-1, 160, 16, 16]             320
InvertedResidual-144          [-1, 160, 16, 16]               0
          Conv2d-145          [-1, 960, 16, 16]         153,600
     BatchNorm2d-146          [-1, 960, 16, 16]           1,920
           ReLU6-147          [-1, 960, 16, 16]               0
          Conv2d-148          [-1, 960, 16, 16]           8,640
     BatchNorm2d-149          [-1, 960, 16, 16]           1,920
           ReLU6-150          [-1, 960, 16, 16]               0
          Conv2d-151          [-1, 320, 16, 16]         307,200
     BatchNorm2d-152          [-1, 320, 16, 16]             640
InvertedResidual-153          [-1, 320, 16, 16]               0
          Conv2d-154         [-1, 1280, 16, 16]         409,600
     BatchNorm2d-155         [-1, 1280, 16, 16]           2,560
           ReLU6-156         [-1, 1280, 16, 16]               0
================================================================
Total params: 2,223,872
Trainable params: 2,223,872
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 3.00
Forward/backward pass size (MB): 798.56
Params size (MB): 8.48
Estimated Total Size (MB): 810.05
----------------------------------------------------------------

Process finished with exit code 0
